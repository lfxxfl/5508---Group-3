{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be58d0b6-c8d1-441a-85a7-3aa996c0c8b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Base URL without the page number\n",
    "base_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711368896&srchtxt=%E9%A8%99%E6%A1%88&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%A8%99%E6%A1%88\"\n",
    "\n",
    "# Define the CSV file to write data to\n",
    "csv_file = \"scraped_data.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL'])  # Writing the headers\n",
    "\n",
    "    # Modify this range to scrape more pages\n",
    "    for page_number in range(1, 26):\n",
    "        # Modify the URL to include the current page number\n",
    "        url = f\"{base_url}&&page={page_number}\"\n",
    "        \n",
    "        # Send a GET request to the page\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.encoding = 'utf-8'  # Ensure correct encoding\n",
    "\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the elements containing the data you're interested in.\n",
    "        posts = soup.find_all('li', class_='pbw')\n",
    "\n",
    "        # Loop through each post and extract data\n",
    "        for post in posts:\n",
    "            # Find the title and URL\n",
    "            title_link = post.find('a')\n",
    "            title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "            url = title_link['href'] if title_link else \"N/A\"\n",
    "            \n",
    "            # The replies and views data\n",
    "            reply_view_element = post.find('p', class_='xg1')\n",
    "            replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "            \n",
    "            # Attempt to capture the date from the correct span element\n",
    "            # We'll look for the span element that contains the date based on the format we expect\n",
    "            date_spans = reply_view_element.find_all('span') if reply_view_element else []\n",
    "            time = \"N/A\"\n",
    "            for span in date_spans:\n",
    "                if '-' in span.get_text():  # A simple check to see if the span might contain a date\n",
    "                    time = span.get_text(strip=True)\n",
    "                    break\n",
    "\n",
    "            # Write the extracted data to the CSV\n",
    "            writer.writerow([title, replies_views, time, url])\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c28b7a80-49b0-4ae5-afb0-dc0ab57b6dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished page 1\n",
      "Finished page 2\n",
      "Finished page 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m post_url\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     60\u001b[0m         post_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.baby-kingdom.com\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 61\u001b[0m     username, main_content, all_replies \u001b[38;5;241m=\u001b[39m scrape_post_details(post_url)\n\u001b[0;32m     63\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow([title, replies_views, time, post_url, username, main_content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(all_replies)])\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m, in \u001b[0;36mscrape_post_details\u001b[1;34m(post_url)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_post_details\u001b[39m(post_url):\n\u001b[1;32m----> 7\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(post_url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m      8\u001b[0m     post_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Extracting the username\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    794\u001b[0m     conn,\n\u001b[0;32m    795\u001b[0m     method,\n\u001b[0;32m    796\u001b[0m     url,\n\u001b[0;32m    797\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    798\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    799\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    800\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    801\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    802\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    803\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    804\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    806\u001b[0m )\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url):\n",
    "    response = requests.get(post_url, headers=headers)\n",
    "    post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extracting the username\n",
    "    username_tag = post_soup.find('a', class_='xw1')\n",
    "    username = username_tag.get_text(strip=True) if username_tag else \"N/A\"\n",
    "    \n",
    "    # Extracting the main content\n",
    "    main_content_tag = post_soup.find('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "    main_content = main_content_tag.get_text(strip=True) if main_content_tag else \"N/A\"\n",
    "    \n",
    "    # Extracting all replies\n",
    "    replies_tags = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "    all_replies = [reply.get_text(strip=True) for reply in replies_tags[1:]] if replies_tags else []  # Skip the first one\n",
    "    \n",
    "    return username, main_content, all_replies\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "base_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711368896&srchtxt=%E9%A8%99%E6%A1%88&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%A8%99%E6%A1%88\"\n",
    "csv_file = \"scraped_data2.csv\"\n",
    "\n",
    "# CSV file setup\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Update headers\n",
    "\n",
    "    # Loop to go through all search result pages\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_url}&page={page_number}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        posts = soup.find_all('li', class_='pbw')\n",
    "        if not posts:\n",
    "            break  # No more posts found\n",
    "        \n",
    "        for post in posts:\n",
    "            title_link = post.find('a')\n",
    "            title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "            post_url = title_link['href'] if title_link else \"N/A\"\n",
    "            \n",
    "            reply_view_element = post.find('p', class_='xg1')\n",
    "            replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "            time = reply_view_element.find('span').get_text(strip=True) if reply_view_element and reply_view_element.find('span') else \"N/A\"\n",
    "            \n",
    "            # Scrape individual post details\n",
    "            if post_url.startswith('/'):\n",
    "                post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "            username, main_content, all_replies = scrape_post_details(post_url)\n",
    "\n",
    "            writer.writerow([title, replies_views, time, post_url, username, main_content, ' | '.join(all_replies)])\n",
    "\n",
    "        print(f\"Finished page {page_number}\")\n",
    "        page_number += 1\n",
    "\n",
    "print(\"Scraping completed and data written to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d41375-607a-4618-8cc8-3c259c685ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m post_url\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     68\u001b[0m         post_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.baby-kingdom.com\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 69\u001b[0m     username, time, main_content, all_replies \u001b[38;5;241m=\u001b[39m scrape_post_details(post_url)\n\u001b[0;32m     71\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow([title, replies_views, time, post_url, username, main_content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(all_replies)])\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished scraping page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m, in \u001b[0;36mscrape_post_details\u001b[1;34m(post_url)\u001b[0m\n\u001b[0;32m     13\u001b[0m post_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Extracting the username\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m username_tag \u001b[38;5;241m=\u001b[39m post_soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthi\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m username \u001b[38;5;241m=\u001b[39m username_tag\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m username_tag \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Extracting the time\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url):\n",
    "    response = requests.get(post_url, headers=headers)\n",
    "    post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extracting the username\n",
    "    username_tag = post_soup.find('div', class_='authi').find('a')\n",
    "    username = username_tag.get_text(strip=True) if username_tag else \"N/A\"\n",
    "    \n",
    "    # Extracting the time\n",
    "    time_tag = post_soup.find('em', id=lambda x: x and x.startswith('authorposton')).find('span')\n",
    "    time = time_tag['title'] if time_tag and 'title' in time_tag.attrs else \"N/A\"\n",
    "    \n",
    "    # Extracting the main content\n",
    "    main_content_tag = post_soup.find('div', class_='pcb').find('div', class_='t_fsz')\n",
    "    main_content = main_content_tag.get_text(strip=True) if main_content_tag else \"N/A\"\n",
    "    \n",
    "    # Extracting replies\n",
    "    replies_tags = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "    replies_text = [reply.get_text(strip=True) for reply in replies_tags[1:]] if replies_tags else []  # Skip the first one which is main content\n",
    "\n",
    "    return username, time, main_content, replies_text\n",
    "\n",
    "# Base URL pattern\n",
    "base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711368896&srchtxt=%E9%A8%99%E6%A1%88&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%A8%99%E6%A1%88\"\n",
    "\n",
    "# CSV file setup\n",
    "with open('scraped_data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Headers\n",
    "\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.encoding = 'utf-8'\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data for page {page_number}, status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        posts = soup.find_all('li', class_='pbw')\n",
    "\n",
    "        if not posts:\n",
    "            print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "            break\n",
    "\n",
    "        for post in posts:\n",
    "            title_link = post.find('a')\n",
    "            title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "            post_url = title_link['href'] if title_link else \"N/A\"\n",
    "\n",
    "            reply_view_element = post.find('p', class_='xg1')\n",
    "            replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "            \n",
    "            # Scrape details from the post's individual page\n",
    "            if post_url.startswith('/'):\n",
    "                post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "            username, time, main_content, all_replies = scrape_post_details(post_url)\n",
    "\n",
    "            writer.writerow([title, replies_views, time, post_url, username, main_content, ' | '.join(all_replies)])\n",
    "\n",
    "        print(f\"Finished scraping page {page_number}.\")\n",
    "        page_number += 1\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f56130e-449a-4856-9a76-1ed17fd1252a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# below is the successful script for 騙案 in BK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8eb5cba-c92c-4fe6-85d4-6f5998d2d45f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "Finished scraping page 3.\n",
      "Finished scraping page 4.\n",
      "Finished scraping page 5.\n",
      "Finished scraping page 6.\n",
      "Finished scraping page 7.\n",
      "Finished scraping page 8.\n",
      "Finished scraping page 9.\n",
      "Finished scraping page 10.\n",
      "Finished scraping page 11.\n",
      "Finished scraping page 12.\n",
      "Finished scraping page 13.\n",
      "Finished scraping page 14.\n",
      "Finished scraping page 15.\n",
      "Finished scraping page 16.\n",
      "Finished scraping page 17.\n",
      "Finished scraping page 18.\n",
      "Finished scraping page 19.\n",
      "Finished scraping page 20.\n",
      "Finished scraping page 21.\n",
      "Finished scraping page 22.\n",
      "Finished scraping page 23.\n",
      "Finished scraping page 24.\n",
      "Finished scraping page 25.\n",
      "No more topics found on page 26. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies\n",
    "        replies_tags = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "        replies_text = [reply.get_text(strip=True) for reply in replies_tags[1:]] if replies_tags else []\n",
    "\n",
    "        return username, time, main_content, replies_text\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"scraped_data4.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711368896&srchtxt=%E9%A8%99%E6%A1%88&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%A8%99%E6%A1%88\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, ' | '.join(all_replies)])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16ebef37-3840-4f02-a6c5-ccc88eb296bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#above ok, but only can scrape first page comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e01313b7-264e-4bbe-a126-f79e612d75e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "Finished scraping page 3.\n",
      "Finished scraping page 4.\n",
      "Finished scraping page 5.\n",
      "Finished scraping page 6.\n",
      "Finished scraping page 7.\n",
      "Finished scraping page 8.\n",
      "Finished scraping page 9.\n",
      "Finished scraping page 10.\n",
      "Finished scraping page 11.\n",
      "Finished scraping page 12.\n",
      "Finished scraping page 13.\n",
      "Finished scraping page 14.\n",
      "Finished scraping page 15.\n",
      "Finished scraping page 16.\n",
      "Finished scraping page 17.\n",
      "Finished scraping page 18.\n",
      "Finished scraping page 19.\n",
      "Finished scraping page 20.\n",
      "Finished scraping page 21.\n",
      "Finished scraping page 22.\n",
      "Finished scraping page 23.\n",
      "Finished scraping page 24.\n",
      "Finished scraping page 25.\n",
      "No more topics found on page 26. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"scraped_data_with_replies.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711368896&srchtxt=%E9%A8%99%E6%A1%88&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%A8%99%E6%A1%88\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8db3491-5cf4-41c6-8996-40d0bb4cdd23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#above successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd10b833-1634-4266-8268-7ac4327b54cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now next keyword:facebook騙案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "942bbd74-340f-42ed-9a8a-7f6f5173306a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "No more topics found on page 2. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"facebook騙案.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711979377&srchtxt=facebook+%E9%A8%99%E6%A1%88&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=facebook+%E9%A8%99%E6%A1%88&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c993fd82-d369-494a-80ec-ac81505acfcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df48f201-aae0-4fdb-a1aa-f26ef44f8586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "No more topics found on page 2. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"scam.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711979650&srchtxt=scam&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=scam&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dc175d5-61c1-4936-b69e-89c4537d9024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#行騙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ace4b7df-20dc-4b8f-98c3-91030ebd206d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "Finished scraping page 3.\n",
      "No more topics found on page 4. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_行騙.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711979851&srchtxt=%E8%A1%8C%E9%A8%99&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E8%A1%8C%E9%A8%99&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e5f7c25-3fc7-4e32-9d4f-d6194a62c575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#呃人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cce6f3af-4d97-4a3e-8236-1f36b3119e57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "Finished scraping page 3.\n",
      "Finished scraping page 4.\n",
      "Finished scraping page 5.\n",
      "Finished scraping page 6.\n",
      "Finished scraping page 7.\n",
      "Finished scraping page 8.\n",
      "Finished scraping page 9.\n",
      "Finished scraping page 10.\n",
      "Finished scraping page 11.\n",
      "Request failed with error ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "Request failed with error ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "Request failed with error Response ended prematurely. Retrying...\n",
      "Finished scraping page 12.\n",
      "Request failed with error ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "Finished scraping page 13.\n",
      "Finished scraping page 14.\n",
      "Finished scraping page 15.\n",
      "Request failed with error ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "Request failed with error Response ended prematurely. Retrying...\n",
      "Request failed with error Response ended prematurely. Retrying...\n",
      "Request failed with error Response ended prematurely. Retrying...\n",
      "Request failed with error Response ended prematurely. Retrying...\n",
      "Request failed with error ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "Request failed with error Response ended prematurely. Retrying...\n",
      "Request failed with error Response ended prematurely. Retrying...\n",
      "Request failed with error Response ended prematurely. Retrying...\n",
      "Request failed with error Response ended prematurely. Retrying...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 123\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_url\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    122\u001b[0m     post_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.baby-kingdom.com\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 123\u001b[0m username, post_time, main_content, all_replies \u001b[38;5;241m=\u001b[39m scrape_post_details(post_url, headers)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Flatten replies into a single string\u001b[39;00m\n\u001b[0;32m    126\u001b[0m all_replies_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(all_replies)\n",
      "Cell \u001b[1;32mIn[39], line 80\u001b[0m, in \u001b[0;36mscrape_post_details\u001b[1;34m(post_url, headers)\u001b[0m\n\u001b[0;32m     77\u001b[0m next_page_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpost_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Make request for the next page\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m response \u001b[38;5;241m=\u001b[39m make_request(next_page_url, headers)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m     82\u001b[0m     post_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 15\u001b[0m, in \u001b[0;36mmake_request\u001b[1;34m(url, headers, retries, delay)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m         response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    794\u001b[0m     conn,\n\u001b[0;32m    795\u001b[0m     method,\n\u001b[0;32m    796\u001b[0m     url,\n\u001b[0;32m    797\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    798\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    799\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    800\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    801\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    802\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    803\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    804\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    806\u001b[0m )\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_呃人.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711980233&srchtxt=%E5%91%83%E4%BA%BA&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E5%91%83%E4%BA%BA&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4afe0d26-48ed-45f5-8c7f-d2bfd63c10f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# above stopped at p16, restart at p17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d007d4a-d644-4ffe-88e2-6a35892b6620",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 17.\n",
      "Finished scraping page 18.\n",
      "Finished scraping page 19.\n",
      "Finished scraping page 20.\n",
      "Finished scraping page 21.\n",
      "Finished scraping page 22.\n",
      "Finished scraping page 23.\n",
      "Finished scraping page 24.\n",
      "Finished scraping page 25.\n",
      "Finished scraping page 26.\n",
      "No more topics found on page 27. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_呃人Part2.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711980233&srchtxt=%E5%91%83%E4%BA%BA&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E5%91%83%E4%BA%BA&&page=17\"\n",
    "\n",
    "    # Start from page 17\n",
    "    page_number = 17\n",
    "\n",
    "    # Scrape process\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc2c2375-908d-44c3-b331-d1a1c6edafad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#呃錢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "286fd8d3-6bcd-4a63-bd3c-691c4ff2424a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "Finished scraping page 3.\n",
      "Finished scraping page 4.\n",
      "Finished scraping page 5.\n",
      "Finished scraping page 6.\n",
      "Finished scraping page 7.\n",
      "Finished scraping page 8.\n",
      "Finished scraping page 9.\n",
      "Finished scraping page 10.\n",
      "Finished scraping page 11.\n",
      "No more topics found on page 12. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_呃錢.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711987964&srchtxt=%E5%91%83%E9%8C%A2&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E5%91%83%E9%8C%A2&&page=1\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "658813b2-6cc3-43a6-8a16-a4d3854a84c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#欺詐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0ece57d-75a2-494e-8ab6-698cc953c827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "Finished scraping page 3.\n",
      "No more topics found on page 4. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_欺詐.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711989099&srchtxt=%E6%AC%BA%E8%A9%90&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E6%AC%BA%E8%A9%90&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a30e5a3b-1794-4a3d-9a02-2a7aaae78af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#欺騙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e26be52-eca3-4864-bf81-9f52fa1aab30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "Finished scraping page 3.\n",
      "Finished scraping page 4.\n",
      "Finished scraping page 5.\n",
      "Finished scraping page 6.\n",
      "Finished scraping page 7.\n",
      "Finished scraping page 8.\n",
      "Finished scraping page 9.\n",
      "Finished scraping page 10.\n",
      "No more topics found on page 11. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_欺騙.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711989441&srchtxt=%E6%AC%BA%E9%A8%99&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E6%AC%BA%E9%A8%99&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbe98f1a-c8fe-420d-94d6-7f926579bcdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#詐騙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2384f5a5-60be-4ffe-9419-efee774875da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "Finished scraping page 3.\n",
      "Finished scraping page 4.\n",
      "Finished scraping page 5.\n",
      "Finished scraping page 6.\n",
      "Finished scraping page 7.\n",
      "Finished scraping page 8.\n",
      "Finished scraping page 9.\n",
      "Finished scraping page 10.\n",
      "Finished scraping page 11.\n",
      "Finished scraping page 12.\n",
      "Finished scraping page 13.\n",
      "No more topics found on page 14. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_詐騙.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711990299&srchtxt=%E8%A9%90%E9%A8%99&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E8%A9%90%E9%A8%99&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e62e56e0-8f24-46cc-8f14-3abf0e4ded8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#電詐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1985da8a-9a0f-42c4-90ad-f9b981c89c5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "No more topics found on page 2. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_電詐.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711991225&srchtxt=%E9%9B%BB%E8%A9%90&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%9B%BB%E8%A9%90&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2127390-1f3d-4fca-9433-60dd247c827f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#電郵騙案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f2eb185-079c-41db-b4a5-90cf618b12fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "No more topics found on page 2. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_電郵騙案.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711991364&srchtxt=%E9%9B%BB%E9%83%B5%E9%A8%99%E6%A1%88&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%9B%BB%E9%83%B5%E9%A8%99%E6%A1%88&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8738bde7-19be-435f-8ea5-0dd08cbfbb31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#電話騙案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1fd06011-aae5-44ab-babf-b35fb963064c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "Finished scraping page 3.\n",
      "Finished scraping page 4.\n",
      "Finished scraping page 5.\n",
      "No more topics found on page 6. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_電話騙案.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711991476&srchtxt=%E9%9B%BB%E8%A9%B1%E9%A8%99%E6%A1%88&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%9B%BB%E8%A9%B1%E9%A8%99%E6%A1%88&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2219eef-78b8-4165-b2ff-3c9c27f5631c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#電騙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b02adf6c-89d2-4c3c-a21c-25c22988fef2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "No more topics found on page 3. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_電騙.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711991900&srchtxt=%E9%9B%BB%E9%A8%99&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%9B%BB%E9%A8%99&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f178d157-9f1f-4614-8ec7-e81203bc692f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#網上情緣騙案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0c983df2-8caa-451b-a777-80b48e512492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "No more topics found on page 2. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_網上情緣騙案.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711992036&srchtxt=%E7%B6%B2%E4%B8%8A%E6%83%85%E7%B7%A3%E9%A8%99%E6%A1%88&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E7%B6%B2%E4%B8%8A%E6%83%85%E7%B7%A3%E9%A8%99%E6%A1%88&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a9bb1de8-e70a-4e01-a568-3c258bb22c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#網上詐騙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bea17674-407c-450a-a556-420a757a6021",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "No more topics found on page 2. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_網上詐騙.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711992330&srchtxt=%E7%B6%B2%E4%B8%8A%E8%A9%90%E9%A8%99&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E7%B6%B2%E4%B8%8A%E8%A9%90%E9%A8%99&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cefe0e29-791e-4ecc-b9f0-4b3a86c20bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#網上騙案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40294e4c-3a92-44fe-bd97-fbb01eea374e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "No more topics found on page 2. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_網上騙案.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711992484&srchtxt=%E7%B6%B2%E4%B8%8A%E9%A8%99%E6%A1%88&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E7%B6%B2%E4%B8%8A%E9%A8%99%E6%A1%88&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9baa58b4-d797-4fe2-b2d7-1d045e7cae76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#騙徒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed26784e-94c2-4134-a3b9-9111cfd64ffe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "Finished scraping page 3.\n",
      "Finished scraping page 4.\n",
      "Finished scraping page 5.\n",
      "Finished scraping page 6.\n",
      "Finished scraping page 7.\n",
      "Finished scraping page 8.\n",
      "Finished scraping page 9.\n",
      "Finished scraping page 10.\n",
      "No more topics found on page 11. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_騙徒.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711992721&srchtxt=%E9%A8%99%E5%BE%92&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%A8%99%E5%BE%92&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8366f9d0-249c-4233-97c8-a3a8982d29e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#騙徒手法層出不窮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e3dc5d57-d152-41c8-84a1-c622897dec72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "No more topics found on page 2. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_騙徒手法層出不窮.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711993452&srchtxt=%E9%A8%99%E5%BE%92%E6%89%8B%E6%B3%95%E5%B1%A4%E5%87%BA%E4%B8%8D%E7%AA%AE&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%A8%99%E5%BE%92%E6%89%8B%E6%B3%95%E5%B1%A4%E5%87%BA%E4%B8%8D%E7%AA%AE&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "16c0e1e0-cfb6-4dfb-9367-f949627e7123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#騙錢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a6bba34e-5cdd-4bed-a2ba-911386699d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "No more topics found on page 3. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_騙錢.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711993609&srchtxt=%E9%A8%99%E9%8C%A2&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%A8%99%E9%8C%A2&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0661bb97-e2a3-43cb-8c49-66da1d9a0e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#騙子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "215a82f9-9074-44a0-9fb5-b5bbbec76048",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "Finished scraping page 3.\n",
      "Finished scraping page 4.\n",
      "Finished scraping page 5.\n",
      "Finished scraping page 6.\n",
      "Finished scraping page 7.\n",
      "Finished scraping page 8.\n",
      "Finished scraping page 9.\n",
      "Finished scraping page 10.\n",
      "Finished scraping page 11.\n",
      "Finished scraping page 12.\n",
      "Finished scraping page 13.\n",
      "Finished scraping page 14.\n",
      "Finished scraping page 15.\n",
      "No more topics found on page 16. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_騙子.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711993715&srchtxt=%E9%A8%99%E5%AD%90&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%A8%99%E5%AD%90&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fefc8ae-d5ee-4ecb-a4f9-5ab2b763429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#騙局"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f0ec16ea-b138-4d94-b521-9a7604b70280",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping page 1.\n",
      "Finished scraping page 2.\n",
      "Finished scraping page 3.\n",
      "Finished scraping page 4.\n",
      "Finished scraping page 5.\n",
      "Finished scraping page 6.\n",
      "Finished scraping page 7.\n",
      "Finished scraping page 8.\n",
      "Finished scraping page 9.\n",
      "Finished scraping page 10.\n",
      "No more topics found on page 11. Ending scrape.\n",
      "Data scraping and CSV writing completed.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Headers to mimic a browser visit\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Function to make requests with retries\n",
    "def make_request(url, headers, retries=5, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}. Retrying...\")\n",
    "                time.sleep(delay)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed with error {e}. Retrying...\")\n",
    "            time.sleep(delay)\n",
    "    return None  # If all retries fail, return None\n",
    "\n",
    "# Function to scrape individual post details\n",
    "def scrape_post_details(post_url, headers):\n",
    "    response = make_request(post_url, headers)\n",
    "    if response:\n",
    "        post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Check if 'authi' class div is present before finding the 'a' tag\n",
    "        authi_div = post_soup.find('div', class_='authi')\n",
    "        if authi_div and authi_div.find('a'):\n",
    "            username_tag = authi_div.find('a')\n",
    "            username = username_tag.get_text(strip=True)\n",
    "        else:\n",
    "            username = \"N/A\"\n",
    "\n",
    "        # Same check for the 'em' tag with 'authorposton' id before finding the 'span' tag\n",
    "        authorposton_em = post_soup.find('em', id=lambda x: x and x.startswith('authorposton'))\n",
    "        if authorposton_em and authorposton_em.find('span'):\n",
    "            time_tag = authorposton_em.find('span')\n",
    "            time = time_tag['title'] if 'title' in time_tag.attrs else \"N/A\"\n",
    "        else:\n",
    "            time = \"N/A\"\n",
    "\n",
    "        # Extracting the main content\n",
    "        pcb_div = post_soup.find('div', class_='pcb')\n",
    "        if pcb_div and pcb_div.find('div', class_='t_fsz'):\n",
    "            main_content_tag = pcb_div.find('div', class_='t_fsz')\n",
    "            main_content = main_content_tag.get_text(strip=True)\n",
    "        else:\n",
    "            main_content = \"N/A\"\n",
    "\n",
    "        # Extracting replies from all pages\n",
    "        replies = []\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Find all reply divs\n",
    "            reply_divs = post_soup.find_all('div', id=lambda x: x and x.startswith('postmessage_'))\n",
    "\n",
    "            # Extract text from reply divs\n",
    "            for reply_div in reply_divs:\n",
    "                reply_text = reply_div.get_text(strip=True)\n",
    "                replies.append(reply_text)\n",
    "\n",
    "            # Find the next page button\n",
    "            next_page_button = post_soup.find('i', class_='ico_on icon-pagin-generic_nextPageOn')\n",
    "\n",
    "            # Break the loop if there's no next page button\n",
    "            if not next_page_button:\n",
    "                break\n",
    "\n",
    "            # Increment page number and get the next page URL\n",
    "            page_number += 1\n",
    "            next_page_url = f\"{post_url}&page={page_number}\"\n",
    "            \n",
    "            # Make request for the next page\n",
    "            response = make_request(next_page_url, headers)\n",
    "            if response:\n",
    "                post_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            else:\n",
    "                break  # Break loop if request fails\n",
    "\n",
    "        return username, time, main_content, replies\n",
    "    else:\n",
    "        return \"N/A\", \"N/A\", \"N/A\", []\n",
    "\n",
    "# CSV file setup\n",
    "csv_file = \"BK_騙局.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Title', 'Replies/Views', 'Time', 'URL', 'Username', 'MainContent', 'Replies'])  # Writing the headers\n",
    "\n",
    "    # Base URL pattern\n",
    "    base_search_url = \"https://www.baby-kingdom.com/search.php?mod=forum&searchid=1711993965&srchtxt=%E9%A8%99%E5%B1%80&orderby=lastpost&ascdesc=desc&searchsubmit=yes&keyword=%E9%A8%99%E5%B1%80&\"\n",
    "\n",
    "    # Scrape process\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        url = f\"{base_search_url}&page={page_number}\"\n",
    "        page_response = make_request(url, headers)\n",
    "        \n",
    "        if page_response:\n",
    "            soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            posts = soup.find_all('li', class_='pbw')\n",
    "            \n",
    "            if not posts:\n",
    "                print(f\"No more topics found on page {page_number}. Ending scrape.\")\n",
    "                break\n",
    "            \n",
    "            for post in posts:\n",
    "                title_link = post.find('a')\n",
    "                title = title_link.get_text(strip=True) if title_link else \"N/A\"\n",
    "                post_url = title_link['href'] if title_link else \"N/A\"\n",
    "                reply_view_element = post.find('p', class_='xg1')\n",
    "                replies_views = reply_view_element.get_text(strip=True) if reply_view_element else \"N/A\"\n",
    "                \n",
    "                # Scrape details from the post's individual page\n",
    "                if post_url.startswith('/'):\n",
    "                    post_url = f\"https://www.baby-kingdom.com{post_url}\"\n",
    "                username, post_time, main_content, all_replies = scrape_post_details(post_url, headers)\n",
    "                \n",
    "                # Flatten replies into a single string\n",
    "                all_replies_str = ' | '.join(all_replies)\n",
    "                \n",
    "                writer.writerow([title, replies_views, post_time, post_url, username, main_content, all_replies_str])\n",
    "\n",
    "            print(f\"Finished scraping page {page_number}.\")\n",
    "            page_number += 1\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for page {page_number}. Skipping...\")\n",
    "            page_number += 1\n",
    "            continue\n",
    "\n",
    "print(\"Data scraping and CSV writing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bbd994-1bc2-44cd-b805-0585a415af7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
