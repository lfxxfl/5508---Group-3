{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00cc65dc-5f10-4f21-b372-45a9639e5516",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated CSV file created at consolidated_LIHKG_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Paths to the CSV files\n",
    "csv_file_paths = {\n",
    "    'facebook騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\facebook騙案.csv\", #1\n",
    "    'fraud': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\fraud.csv\", #2\n",
    "    'ig騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\ig騙案.csv\", #3\n",
    "    'scam': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\scam.csv\", #4\n",
    "    '交友app騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\交友app騙案.csv\",  #5\n",
    "    '行騙': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\行騙.csv\", #6\n",
    "    '呃': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\呃.csv\", #7\n",
    "    '呃人': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\呃人.csv\", #8\n",
    "    '呃錢': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\呃錢.csv\", #9\n",
    "    '欺詐': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\欺詐.csv\", #10\n",
    "    '欺騙': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\欺騙.csv\", #11\n",
    "    '詐騙': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\詐騙.csv\", #12\n",
    "    '電詐': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\電詐.csv\", #13\n",
    "    '電郵騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\電郵騙案.csv\", #14\n",
    "    '電話騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\電話騙案.csv\", #15\n",
    "    '電騙': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\電騙.csv\", #16\n",
    "    '網上呃錢': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\網上呃錢.csv\", #17\n",
    "    '網上情緣騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\網上情緣騙案.csv\", #18\n",
    "    '網上詐騙': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\網上詐騙.csv\",  #19\n",
    "    '網上騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\網上騙案.csv\", #20\n",
    "    '騙徒': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\騙徒.csv\", #21\n",
    "    '騙徒手法層出不窮': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\騙徒手法層出不窮.csv\", #22\n",
    "    '騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\騙案.csv\", #23\n",
    "    '騙錢': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\騙錢.csv\" #24\n",
    "}\n",
    "\n",
    "# The combined data\n",
    "combined_data = []\n",
    "\n",
    "# Iterate over the CSV files and read them\n",
    "for keyword, file_path in csv_file_paths.items():\n",
    "    with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            row['Keyword'] = keyword  # Add the keyword to each row\n",
    "            combined_data.append(row)\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv_file_path = 'consolidated_LIHKG_data.csv'\n",
    "\n",
    "# Writing the combined data to the new CSV file\n",
    "with open(output_csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    # Assuming all CSVs have the same field names, we can get the field names from the first CSV\n",
    "    fieldnames = list(combined_data[0].keys())\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in combined_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Consolidated CSV file created at {output_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaa1743d-96eb-46ae-b40e-1d0446cc1e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def reformat_csv(input_csv_path, output_csv_path):\n",
    "    with open(input_csv_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_csv_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.DictReader(infile)\n",
    "        # Assuming 'All Replies' is the name of the column that contains the replies.\n",
    "        fieldnames = reader.fieldnames + ['Reply Number', 'Reply Content']  \n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            # Splitting by the reply number\n",
    "            replies = re.split(r\"(#\\d+)\", row['All Replies'])  \n",
    "            reply_number = \"\"\n",
    "            for content in replies:\n",
    "                if content.startswith(\"#\"):\n",
    "                    reply_number = content  # This is the reply number\n",
    "                elif content.strip():  # Check if content is not just whitespace\n",
    "                    new_row = row.copy()\n",
    "                    new_row['Reply Number'] = reply_number\n",
    "                    new_row['Reply Content'] = content.strip()\n",
    "                    writer.writerow(new_row)\n",
    "\n",
    "# Use the function with the actual file paths as arguments\n",
    "reformat_csv(r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\consolidated_LIHKG_data.csv\", \n",
    "             r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\try_consolidated_LIHKG_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62673fa0-c84c-43dc-9fde-57d79942b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# above script generated a very large csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6eba38-999f-4439-a697-234e8289d0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed file created at C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\try_consolidated_LIHKG_data.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def compress_csv(csv_file_path):\n",
    "    # Assumes the '.csv' in the file path; updates this as needed.\n",
    "    zip_file_path = csv_file_path.replace('.csv', '.zip')\n",
    "    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(csv_file_path, os.path.basename(csv_file_path))\n",
    "    return zip_file_path\n",
    "\n",
    "# Call the function with the path to your large CSV file\n",
    "compressed_file_path = compress_csv(r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\try_consolidated_LIHKG_data.csv\")\n",
    "print(f\"Compressed file created at {compressed_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99e0b5c-40e8-46a1-8fff-2fac98ddf04d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "# This function will read the current data and place each reply in the same column but different rows\n",
    "def reformat_csv(input_csv_path, output_csv_path):\n",
    "    with open(input_csv_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_csv_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames + ['Reply']  # Add a new field for the reply\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            all_replies = row['All Replies']  # This should match the exact name of your column\n",
    "            replies = re.split(r\"#\\d+\", all_replies)  # Split by the reply number\n",
    "            for reply in filter(None, replies):  # Filter out any empty strings\n",
    "                new_row = row.copy()\n",
    "                new_row['All Replies'] = reply.strip()  # Place the reply in the 'All Replies' column\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "# Replace 'input.csv' and 'output.csv' with your actual file paths\n",
    "reformat_csv(r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\consolidated_LIHKG_data.csv\", r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\try2_consolidated_LIHKG_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e5606a1-0e83-4ec4-8eaa-6457998fb033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def reformat_csv(input_csv_path, output_csv_path):\n",
    "    with open(input_csv_path, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "\n",
    "        # First pass to find the max number of replies\n",
    "        max_replies = 0\n",
    "        for row in reader:\n",
    "            replies = re.findall(r\"(#\\d+)\", row['All Replies'])\n",
    "            # Subtract 1 to exclude the main content (Reply 1)\n",
    "            if len(replies) > 1 and (len(replies) - 1 > max_replies): \n",
    "                max_replies = len(replies) - 1\n",
    "\n",
    "        infile.seek(0)  # Reset the read position of the CSV file to the beginning\n",
    "        next(reader)  # Skip the header row for the second read through\n",
    "\n",
    "        # Create new fieldnames for the replies, starting with 'Reply 2'\n",
    "        fieldnames = [fieldname for fieldname in reader.fieldnames if fieldname != 'All Replies'] + [f'Reply {i+2}' for i in range(max_replies)]\n",
    "\n",
    "        with open(output_csv_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                comments = re.split(r\"(#\\d+)\", row['All Replies'])\n",
    "                # Skip the first reply (main content) and store the rest\n",
    "                reply_content = [comment.strip() for comment in comments if not comment.startswith(\"#\") and comment.strip()][1:]\n",
    "\n",
    "                # Create a new row without the 'All Replies' column and with empty fields for all replies\n",
    "                new_row = {key: row[key] for key in row if key != 'All Replies'}\n",
    "                new_row.update({f'Reply {i+2}': '' for i in range(max_replies)})\n",
    "                # Update the new_row with the actual reply content\n",
    "                new_row.update({f'Reply {i+2}': reply_content[i] for i in range(len(reply_content))})\n",
    "\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "# Replace the paths with your actual CSV file paths\n",
    "\n",
    "reformat_csv(r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\consolidated_LIHKG_data.csv\", r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\try2_consolidated_LIHKG_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09bed11-e7f8-4618-b9e0-dd751906e5f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Replies' removed. New CSV file created at 'new_csv_file.csv'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\formatted_consolidated_Babykingdom_data.csv\"\n",
    "\n",
    "# New CSV file path (without the 'Replies' column)\n",
    "new_csv_file_path = 'new_csv_file.csv'\n",
    "\n",
    "# Column to remove\n",
    "column_to_remove = 'Replies'\n",
    "\n",
    "# Read the CSV file and remove the specified column\n",
    "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "     open(new_csv_file_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = [field for field in reader.fieldnames if field != column_to_remove]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in reader:\n",
    "        del row[column_to_remove]\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Column '{column_to_remove}' removed. New CSV file created at '{new_csv_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091e47d0-f45c-432c-94bd-4e14d0b872e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file cleaned and saved at 'cleaned_csv_file.csv'\n",
      "Number of rows cleaned: 3229\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\formatted_consolidated_Babykingdom_data.csv\"\n",
    "df = pd.read_csv(csv_file_path, low_memory=False)  # Set low_memory=False to suppress DtypeWarning\n",
    "\n",
    "# Store the initial number of rows\n",
    "initial_rows = len(df)\n",
    "\n",
    "# Clean the data\n",
    "# Drop rows with missing main content\n",
    "df = df.dropna(subset=['MainContent'])\n",
    "\n",
    "# Drop rows with only one Chinese word in main content\n",
    "df = df[df['MainContent'].apply(lambda x: len(x.split()) >= 2)]\n",
    "\n",
    "# Drop rows with less than three English words in main content\n",
    "df = df[df['MainContent'].apply(lambda x: len([w for w in x.split() if w.isalpha()]) >= 3)]\n",
    "\n",
    "# Check if any reply columns exist before applying cleaning operations\n",
    "reply_columns = [col for col in df.columns if col.startswith('Reply_')]\n",
    "if reply_columns:\n",
    "    # Concatenate all reply columns into a single column\n",
    "    df['Replies'] = df[reply_columns].apply(lambda x: ' | '.join(x.dropna()), axis=1)\n",
    "\n",
    "    # Drop rows with replies containing only \"推\" or \"推\" + symbols\n",
    "    df['Replies'] = df['Replies'].apply(lambda x: '' if x.startswith('推') and len(x) <= 10 else x)\n",
    "    \n",
    "    # Drop rows with less than two Chinese words in replies\n",
    "    df = df[df['Replies'].apply(lambda x: len(x.split()) >= 2)]\n",
    "\n",
    "# Drop rows with less than three English words in replies\n",
    "df = df[df['Replies'].apply(lambda x: len([w for w in x.split() if w.isalpha()]) >= 3)]\n",
    "\n",
    "# Drop duplicate rows based on author, topic, and time\n",
    "df = df.drop_duplicates(subset=['Username', 'Title', 'Time'])\n",
    "\n",
    "# Calculate the number of cleaned rows\n",
    "cleaned_rows = initial_rows - len(df)\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "cleaned_csv_file_path = 'cleaned_csv_file.csv'\n",
    "df.to_csv(cleaned_csv_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file cleaned and saved at '{cleaned_csv_file_path}'\")\n",
    "print(f\"Number of rows cleaned: {cleaned_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81352f0c-e0b4-4328-9fd1-57642d44a60a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file cleaned and saved at 'cleaned_csv_file_LIHKG.csv'\n",
      "Number of rows cleaned: 33\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\LIHKG\\formatted_consolidated_LIHKG_data.csv\"\n",
    "df = pd.read_csv(csv_file_path, low_memory=False)  # Set low_memory=False to suppress DtypeWarning\n",
    "\n",
    "# Store the initial number of rows\n",
    "initial_rows = len(df)\n",
    "\n",
    "# Clean the data based on specified criteria\n",
    "# Criteria 1: \"美斯\" appears in Title row\n",
    "df = df[~df['Title'].str.contains('美斯')]\n",
    "\n",
    "# Criteria 2: \"運動減肥\" appears in Title row\n",
    "df = df[~df['Title'].str.contains('運動減肥')]\n",
    "\n",
    "# Criteria 3: \"23條\" appears in Title row\n",
    "df = df[~df['Title'].str.contains('23條')]\n",
    "\n",
    "# Criteria 4: \"呃like\" appears in Title row\n",
    "df = df[~df['Title'].str.contains('呃like')]\n",
    "\n",
    "# Criteria 5: \"邁阿密\" appears in Title row\n",
    "df = df[~df['Title'].str.contains('邁阿密')]\n",
    "\n",
    "# Criteria 6: \"屈穎妍\" appears in Title row\n",
    "df = df[~df['Title'].str.contains('屈穎妍')]\n",
    "\n",
    "# Criteria 7: \"利物浦\" appears in Title row\n",
    "df = df[~df['Title'].str.contains('利物浦')]\n",
    "\n",
    "# Calculate the number of cleaned rows\n",
    "cleaned_rows = initial_rows - len(df)\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "cleaned_csv_file_path = 'cleaned_csv_file_LIHKG.csv'\n",
    "df.to_csv(cleaned_csv_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file cleaned and saved at '{cleaned_csv_file_path}'\")\n",
    "print(f\"Number of rows cleaned: {cleaned_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c887c49-18c2-4a9a-a7ee-c37c575c4f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
