{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d46e2b-cc6b-42f6-8591-edbc1863a556",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated CSV file created at consolidated_Babykingdom_data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Increase the field size limit\n",
    "csv.field_size_limit(10**6)\n",
    "\n",
    "# Paths to the CSV files\n",
    "csv_file_paths = {\n",
    "    'facebook騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_facebook騙案.csv\", #1\n",
    "    'scam': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_scam.csv\", #2\n",
    "    '行騙': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_行騙.csv\", #3\n",
    "    '呃人': [r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_呃人.csv\", r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_呃人Part2.csv\"], #4\n",
    "    '呃錢': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_呃錢.csv\",    #5\n",
    "    '欺詐': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_欺詐.csv\", #6\n",
    "    '欺騙': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_欺騙.csv\", #7\n",
    "    '詐騙': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_詐騙.csv\", #8\n",
    "    '電詐': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_電詐.csv\", #9\n",
    "    '電郵騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_電郵騙案.csv\", #10\n",
    "    '電話騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_電話騙案.csv\", #11\n",
    "    '電騙': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_電騙.csv\", #12\n",
    "    '網上情緣騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_網上情緣騙案.csv\", #13\n",
    "    '網上詐騙': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_網上詐騙.csv\", #14\n",
    "    '網上騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_網上騙案.csv\", #15\n",
    "    '騙子': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_騙子.csv\", #16\n",
    "    '騙局': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_騙局.csv\", #17\n",
    "    '騙徒': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_騙徒.csv\", #18\n",
    "    '騙徒手法層出不窮': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_騙徒手法層出不窮.csv\",  #19\n",
    "    '騙案': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_騙案.csv\", #20\n",
    "    '騙錢': r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\BK_騙錢.csv\" #21\n",
    "}\n",
    "\n",
    "# The combined data\n",
    "combined_data = []\n",
    "\n",
    "# Iterate over the CSV files and read them\n",
    "for keyword, file_paths in csv_file_paths.items():\n",
    "    # If the file paths are not in a list format, convert to a list\n",
    "    if not isinstance(file_paths, list):\n",
    "        file_paths = [file_paths]\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                row['Keyword'] = keyword  # Add the keyword to each row\n",
    "                combined_data.append(row)\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv_file_path = 'consolidated_Babykingdom_data.csv'\n",
    "\n",
    "# Writing the combined data to the new CSV file\n",
    "with open(output_csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    # Assuming all CSVs have the same field names, we can get the field names from the first CSV\n",
    "    fieldnames = list(combined_data[0].keys())\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in combined_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Consolidated CSV file created at {output_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "043d3615-51c1-4b8e-b382-67f8520c83e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated CSV file created at consolidated_Babykingdom_data_with_replies.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Increase the field size limit\n",
    "csv.field_size_limit(10**6)\n",
    "\n",
    "# Path to the combined CSV file\n",
    "csv_file_path = r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\consolidated_Babykingdom_data.csv\"\n",
    "\n",
    "# The combined data\n",
    "combined_data = []\n",
    "\n",
    "# Read the combined CSV file\n",
    "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        # Split the \"Replies\" row based on \"|\" and create new columns up to a limit\n",
    "        replies = row['Replies'].split('|')\n",
    "        max_replies = 10  # Limit the number of new reply columns\n",
    "        for i, reply in enumerate(replies[:max_replies], start=1):\n",
    "            row[f'Reply_{i}'] = reply.strip()\n",
    "        \n",
    "        # Remove the original \"Replies\" row\n",
    "        del row['Replies']\n",
    "        \n",
    "        combined_data.append(row)\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv_file_path = 'consolidated_Babykingdom_data_with_replies.csv'\n",
    "\n",
    "# Writing the combined data to the new CSV file\n",
    "with open(output_csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    # Assuming all CSVs have the same field names, we can get the field names from the first CSV\n",
    "    fieldnames = list(combined_data[0].keys())\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in combined_data:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Consolidated CSV file created at {output_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba9602c7-eb39-47e5-8579-1954855237b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated CSV file created at consolidated_Babykingdom_data_with_replies.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Increase the field size limit\n",
    "csv.field_size_limit(10**6)\n",
    "\n",
    "# Path to the combined CSV file\n",
    "csv_file_path = r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\formatted_consolidated_Babykingdom_data.csv\"\n",
    "\n",
    "# The combined data\n",
    "combined_data = []\n",
    "\n",
    "# Read the combined CSV file\n",
    "with open(csv_file_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    max_replies = 0  # Initialize maximum number of replies\n",
    "    for row in reader:\n",
    "        # Split the \"Replies\" row based on \"|\"\n",
    "        replies = row['Replies'].split('|')\n",
    "        num_replies = len(replies)\n",
    "        max_replies = max(max_replies, num_replies)  # Update maximum number of replies\n",
    "        row['Replies'] = replies  # Store replies as a list in the row\n",
    "        combined_data.append(row)\n",
    "\n",
    "# Define the reply column names\n",
    "reply_column_names = [f\"Reply_{i+1}\" for i in range(max_replies)]\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv_file_path = 'consolidated_Babykingdom_data_with_replies.csv'\n",
    "\n",
    "# Writing the combined data to the new CSV file\n",
    "with open(output_csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "    # Write the header\n",
    "    fieldnames = list(combined_data[0].keys()) + reply_column_names\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write rows to the CSV file\n",
    "    for row in combined_data:\n",
    "        # Add reply columns to the row\n",
    "        for i, reply in enumerate(row['Replies'], start=1):\n",
    "            row[f\"Reply_{i}\"] = reply.strip()\n",
    "        del row['Replies']  # Remove the original \"Replies\" row\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Consolidated CSV file created at {output_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb1336e6-4058-4e9b-94db-258d0b8a509c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file cleaned and saved at 'cleaned_csv_file.csv'\n",
      "Number of rows cleaned: 337\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = r\"C:\\Users\\user\\Desktop\\Miki\\5508\\Project\\Scraped data\\Babykingdom\\formatted_consolidated_Babykingdom_data.csv\"\n",
    "df = pd.read_csv(csv_file_path, low_memory=False)  # Set low_memory=False to suppress DtypeWarning\n",
    "\n",
    "# Store the initial number of rows\n",
    "initial_rows = len(df)\n",
    "\n",
    "# Clean the data\n",
    "# Drop rows with missing main content\n",
    "df = df.dropna(subset=['MainContent'])\n",
    "\n",
    "# Drop rows with main content shown as 'N/A', 'NA', or 'Null'\n",
    "df = df[~df['MainContent'].str.lower().isin(['n/a', 'na', 'null'])]\n",
    "\n",
    "# Iterate over each group of exactly same author and topic\n",
    "for _, group in df.groupby(['Username', 'Title']):\n",
    "    # Check if any row in the group has \"0個評論\" in the Replies/Views row\n",
    "    if '0個評論' in group['Replies/Views'].values:\n",
    "        # Drop rows with \"0個評論\" in the Replies/Views row\n",
    "        df = df.drop(group.index)\n",
    "\n",
    "# Iterate over each group of exact same topic and time\n",
    "for _, group in df.groupby(['Title', 'Time']):\n",
    "    # Check if any row in the group has \"0個評論\" in the Replies/Views row\n",
    "    if '0個評論' in group['Replies/Views'].values:\n",
    "        # Drop rows with \"0個評論\" in the Replies/Views row\n",
    "        df = df.drop(group.index)\n",
    "\n",
    "# Clean all cells in Reply_1 to Reply_2000 with only a word \"推\" or \"推\" + some symbols\n",
    "for col in df.columns:\n",
    "    if col.startswith('Reply_'):\n",
    "        df[col] = df[col].apply(lambda x: '' if str(x).startswith('推') and len(str(x)) <= 10 else x)\n",
    "\n",
    "# Calculate the number of cleaned rows\n",
    "cleaned_rows = initial_rows - len(df)\n",
    "\n",
    "# Save the cleaned DataFrame back to a CSV file\n",
    "cleaned_csv_file_path = 'cleaned_csv_file.csv'\n",
    "df.to_csv(cleaned_csv_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file cleaned and saved at '{cleaned_csv_file_path}'\")\n",
    "print(f\"Number of rows cleaned: {cleaned_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb9732-448d-41aa-bce6-fa4bdb4a9780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
